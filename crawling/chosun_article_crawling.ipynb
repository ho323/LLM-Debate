{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492d55db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[페이지 21] https://www.chosun.com/nsearch/?query=소비쿠폰&page=21&siteid=&sort=1&date_period=all&date_start=&date_end=&writer=&field=&emd_word=&expt_word=&opt_chk=false&app_check=0&website=www,chosun&category=\n",
      "✅ 수집됨 | URL: https://www.chosun.com/politics/politics_general/2025/06/20/66UVA2RPVZE3TEPIQNBXXMYIDU/\n",
      "제목: 재래시장 깜짝 방문한 李대통령 “주가 많이 올랐죠?”\n",
      "날짜: 입력 2025.06.20. 18:10\n",
      "본문: 이재명 대통령이 2...\n",
      "\n",
      "✅ 수집됨 | URL: http://woman.chosun.com/news/articleView.html?idxno=121608\n",
      "제목: [키워드로 읽는 경제 트렌드] 새 정부소비쿠폰, 누가 언제 얼마 받나\n",
      "날짜: \n",
      "본문: ...\n",
      "\n",
      "✅ 수집됨 | URL: https://biz.chosun.com/stock/stock_general/2025/06/20/DZX5KBSWL5DWDEREQX3SKLSSAY/\n",
      "제목: [마켓뷰] 30兆 추경, 내수 살릴까… 중동 리스크에도 3000선 뚫은 코스피\n",
      "날짜: 입력 2025.06.20. 16:33\n",
      "본문: 코스피지수가 202...\n",
      "\n",
      "✅ 수집됨 | URL: https://biz.chosun.com/policy/policy_sub/2025/06/20/UXYY6H2AKNH55ALGNXRRKUTAVI/\n",
      "제목: [새정부 추경]소비쿠폰에 10兆, AI 전환에 투자… 새 정부 추경, 필수 추경과 차이점은\n",
      "날짜: 입력 2025.06.20. 16:24\n",
      "본문: 이재명 정부의 30...\n",
      "\n",
      "✅ 수집됨 | URL: https://biz.chosun.com/policy/politics/president_office/2025/06/20/KPKOYFIDMZEMNFFRRPZGQOQXWU/\n",
      "제목: “내가 성남시장때”… 추경 국무회의서 ‘민원’ 작심발언 한 李\n",
      "날짜: 입력 2025.06.20. 16:06\n",
      "본문: “제가 성남시장 취...\n",
      "\n",
      "✅ 수집됨 | URL: http://weekly.chosun.com/news/articleView.html?idxno=42755\n",
      "제목: \"나 없어서 좋았다면서요?\" 비서실장에 건넨 李 농담 화제\n",
      "날짜: \n",
      "본문: ...\n",
      "\n",
      "✅ 수집됨 | URL: https://biz.chosun.com/policy/policy_sub/2025/06/20/O6L3HGIR7VHO5CF2IRQKB3ZKLQ/\n",
      "제목: [새정부 추경] 할인 쿠폰은 선착순 … 영화는 4번, 전시는 5번까지\n",
      "날짜: 입력 2025.06.20. 10:00\n",
      "본문: 정부가 국내 소비...\n",
      "\n",
      "✅ 수집됨 | URL: https://biz.chosun.com/stock/stock_general/2025/06/20/3R4C42DDMRDYFFRVRPTELA4ANE/\n",
      "제목: 코스피, 관망세 유입에 2980대 ‘숨고르기’\n",
      "날짜: 입력 2025.06.20. 09:19\n",
      "본문: 20일 한국 증시는...\n",
      "\n",
      "✅ 수집됨 | URL: http://weekly.chosun.com/news/articleView.html?idxno=42734\n",
      "제목: 송언석 \"전 국민소비쿠폰, 포퓰리즘 정권 데뷔쇼\"\n",
      "날짜: \n",
      "본문: ...\n",
      "\n",
      "✅ 수집됨 | URL: https://www.chosun.com/economy/economy_general/2025/06/20/W5BG52UCBBCXREZC5OGPPUB5WU/\n",
      "제목: 4인 가족 평균 100만원 지원… 지역화폐·카드 중 선택\n",
      "날짜: 입력 2025.06.20. 00:55\n",
      "본문: 정부가 19일 발표...\n",
      "\n",
      "⏩ archive URL 스킵됨: https://archive.chosun.com/pdf/i_service/read_pdf_s.jsp?PDF=20250620A04JH4&Y=2025&M=06\n",
      "⏩ archive URL 스킵됨: https://archive.chosun.com/pdf/i_service/read_pdf_s.jsp?PDF=20250620A04JH4&Y=2025&M=06\n",
      "\n",
      "✅ 총 10개의 기사 저장 완료: chosun_articles_sobikupon_21.json\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "# 저장 이름 설정\n",
    "OUTPUT_FILE = \"chosun_articles_sobikupon.json\"\n",
    "\n",
    "# 검색 결과 URL 생성\n",
    "# 1번째 페이지와 그 이후 페이지 url 구성이 달라 조건문으로 분리\n",
    "def get_search_url(page):\n",
    "    if page == 1:\n",
    "        return \"https://www.chosun.com/nsearch/?query=소비쿠폰\"\n",
    "    else:\n",
    "        return (\n",
    "            f\"https://www.chosun.com/nsearch/?query=소비쿠폰\"\n",
    "            f\"&page={page}\"\n",
    "            f\"&siteid=&sort=1&date_period=all&date_start=&date_end=\"\n",
    "            f\"&writer=&field=&emd_word=&expt_word=&opt_chk=false&app_check=0\"\n",
    "            f\"&website=www,chosun&category=\"\n",
    "        )\n",
    "\n",
    "# 기사 본문 추출 함수\n",
    "def get_article_content(article_url, soup):\n",
    "    content = \"\"\n",
    "    ## url도메인마다 태그가 달라 이를 분리해줌\n",
    "    ## 총 3가지로 분리\n",
    "    if article_url.startswith(\"https://woman.chosun.com/\"):\n",
    "        article_tag = soup.select_one(\"article\")\n",
    "        if article_tag:\n",
    "            p_tags = article_tag.find_all(\"p\")\n",
    "            content = \"\\n\".join(p.get_text(strip=True) for p in p_tags if p.get_text(strip=True))\n",
    "\n",
    "    elif article_url.startswith(\"https://weekly.chosun.com/\") or \\\n",
    "         article_url.startswith(\"https://topclass.chosun.com/\") or \\\n",
    "         article_url.startswith(\"https://it.chosun.com/\"):\n",
    "        article_tag = soup.select_one(\"article#article-view-content-div\")\n",
    "        if article_tag:\n",
    "            p_tags = article_tag.find_all(\"p\")\n",
    "            content = \"\\n\".join(p.get_text(strip=True) for p in p_tags if p.get_text(strip=True))\n",
    "\n",
    "    else:\n",
    "        content_tags = soup.select(\"p.article-body__content.article-body__content-text\")\n",
    "        content = \"\\n\".join(p.get_text(strip=True) for p in content_tags if p.get_text(strip=True))\n",
    "\n",
    "    return content\n",
    "\n",
    "# 날짜 수집 함수\n",
    "def get_article_date(article_url, soup):\n",
    "    date_text = \"\"\n",
    "    try:\n",
    "        if article_url.startswith(\"https://woman.chosun.com/\"):\n",
    "            li = soup.select_one(\"div.info-group article.item ul.infomation li:nth-of-type(2)\")\n",
    "            if li:\n",
    "                date_text = li.get_text(strip=True)\n",
    "\n",
    "        elif article_url.startswith(\"https://weekly.chosun.com/\") or \\\n",
    "             article_url.startswith(\"https://topclass.chosun.com/\"):\n",
    "            li = soup.select_one(\"div.head-inner ul.infomation li:nth-of-type(2)\")\n",
    "            if li:\n",
    "                date_text = li.get_text(strip=True)\n",
    "\n",
    "        elif article_url.startswith(\"https://it.chosun.com/\"):\n",
    "            li = soup.select_one(\"ul.infomation li:nth-of-type(2)\")\n",
    "            if li:\n",
    "                date_text = li.get_text(strip=True)\n",
    "\n",
    "        else:\n",
    "            date_span = soup.select_one(\"span.inputDate\")\n",
    "            if date_span:\n",
    "                date_text = date_span.get_text(strip=True)\n",
    "    except Exception as e: #예외처리\n",
    "        print(f\"날짜 수집 중 오류: {e}\")\n",
    "\n",
    "    return date_text\n",
    "\n",
    "# 브라우저 설정\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "driver.set_page_load_timeout(180)  \n",
    "\n",
    "all_articles = []\n",
    "visited_urls = set()\n",
    "\n",
    "# 1~20페이지 반복\n",
    "for page in range(21, 22):\n",
    "    url = get_search_url(page)\n",
    "    print(f\"\\n[페이지 {page}] {url}\")\n",
    "    try:\n",
    "        driver.get(url)\n",
    "    except Exception as e:\n",
    "        print(f\" 페이지 {page} 로딩 실패: {e}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div.story-card__headline > a\"))\n",
    "        )\n",
    "    except:\n",
    "        print(f\" 페이지 {page}: 기사 로딩 실패\")\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    link_tags = soup.select(\"div.story-card__headline > a\")\n",
    "\n",
    "    if not link_tags:\n",
    "        print(f\" 페이지 {page}에서 기사 없음\")\n",
    "        continue\n",
    "\n",
    "    for tag in link_tags:\n",
    "        try:\n",
    "            title = tag.get_text(strip=True)\n",
    "            article_url = tag[\"href\"]\n",
    "            if not article_url.startswith(\"http\"):\n",
    "                article_url = \"https://www.chosun.com\" + article_url\n",
    "\n",
    "            #  archive URL 스킵 조건 추가\n",
    "            if article_url.startswith(\"https://archive.chosun.com\"):\n",
    "                print(f\"⏩ archive URL 스킵됨: {article_url}\")\n",
    "                continue\n",
    "\n",
    "            if article_url in visited_urls:\n",
    "                continue\n",
    "            visited_urls.add(article_url)\n",
    "\n",
    "            # 기사 본문 페이지 이동 및 수집\n",
    "            try:\n",
    "                driver.get(article_url)\n",
    "                WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, \"article, p.article-body__content\"))\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"  기사 페이지 로딩 실패 (타임아웃 또는 로딩 실패): {e} | URL: {article_url}\")\n",
    "                continue\n",
    "\n",
    "            time.sleep(2.5)\n",
    "            article_soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "            # 본문 및 날짜 내용 추출\n",
    "            content = get_article_content(article_url, article_soup)\n",
    "            date = get_article_date(article_url, article_soup)\n",
    "\n",
    "            # 저장\n",
    "            article_data = {\n",
    "                \"title\": title,\n",
    "                \"url\": article_url,\n",
    "                \"date\": date,\n",
    "                \"content\": content\n",
    "            }\n",
    "            all_articles.append(article_data)\n",
    "\n",
    "            # 출력\n",
    "            content_preview = content[:10].replace(\"\\n\", \" \").strip()\n",
    "            print(f\" 수집됨 | URL: {article_url}\")\n",
    "            print(f\"제목: {title}\")\n",
    "            print(f\"날짜: {date}\")\n",
    "            print(f\"본문: {content_preview}...\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" 오류 발생: {e}\")\n",
    "\n",
    "# 브라우저 종료 및 저장\n",
    "driver.quit()\n",
    "\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_articles, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n 총 {len(all_articles)}개의 기사 저장 완료: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12f5aff",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
